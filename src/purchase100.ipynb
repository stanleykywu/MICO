{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Competition (MICO) @ IEEE SatML 2023: Purchase-100\n",
    "\n",
    "Welcome to the MICO competition!\n",
    "\n",
    "This notebook will walk you through the process of creating and packaging a submission to one of the challenges.\n",
    "\n",
    "Let's start by downloading and extracting the archive for the Purchase-100 challenge.\n",
    "\n",
    "**NOTE**: Public anonymous access to the competition data is disabled. \n",
    "Upon registering for the competition, you will be shown a URL with an embedded bearer token that you must use instead of the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://membershipinference.blob.core.windows.net/mico/purchase100.zip?si=purchase100&spr=https&sv=2021-06-08&sr=b&sig=YzJUTPoNndtIy0y2666XnPXS4WBF%2BbN7kbVM2soQNoU%3D to ./purchase100.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37fb912db4748bfa6e525787a32fde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201262523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./purchase100.zip to .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "url = \"https://membershipinference.blob.core.windows.net/mico/purchase100.zip?si=purchase100&spr=https&sv=2021-06-08&sr=b&sig=YzJUTPoNndtIy0y2666XnPXS4WBF%2BbN7kbVM2soQNoU%3D\"\n",
    "filename = \"purchase100.zip\"\n",
    "md5 = \"67eba1f88d112932fe722fef85fb95fd\"\n",
    "\n",
    "try:\n",
    "    download_and_extract_archive(url=url, download_root=os.curdir, extract_root=None, filename=filename, md5=md5, remove_finished=False)\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e)\n",
    "    print(\"Have you replaced the URL above with the one you got after registering?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "The archive was extracted under the `purchase100` folder containing 3 sub-folders, one for each of the scenarios in the challenge:\n",
    "\n",
    "- `purchase100_lo`  : Models trained with DP-SGD and a small privacy budget ($\\epsilon \\approx 4$) \n",
    "- `purchase100_hi`  : Models trained with DP-SGD and a large privacy budget ($\\epsilon \\approx 10$) \n",
    "- `purchase100_inf` : Models trained without differential privacy guarantee ($\\epsilon = \\infty$)\n",
    "\n",
    "Each of these folders contains 3 other folders:\n",
    "\n",
    "- `train`: Models with metadata allowing to reconstruct their full training datasets. Use these to develop your attacks without having to train your own models.\n",
    "- `dev`: Models with metadata allowing to reconstruct just the set of challenge examples. Membership predictions for these challenges will be used to evaluate submissions during the competition and update the live scoreboard in CodaLab. \n",
    "- `final`: Models with metadata allowing to reconstruct just the set of challenge examples. Membership predictions for these challenges will be used to evaluate submissions when the competition closes and to determine the final ranking.\n",
    "\n",
    "Each model folder in `train`, `dev`, and `final` contains a `model.pt` file with the model weights (a serialized PyTorch `state_dict`). There are 100 models in `train`, and 50 models in each of `dev` and `final`.\n",
    "\n",
    "Models in the `train` folder come with 3 PRNG seeds used to reconstruct the set of member and non-member challenge examples, and the rest of the examples in the training dataset of the model. Additionally (and redundantly), a `solution.csv` file reveals the membership information of the challenge examples.\n",
    "\n",
    "Models in the `dev` and `final` folders contain just 1 PRNG seed used to reconstruct the set of challenge examples, without revealing which were included in the training dataset.\n",
    "\n",
    "We provide utilities to reconstruct the different data splits from provided seeds and to load models as classes inheriting from `torch.nn.Module`. If you use TensorFlow, JAX, or any other framework, you can easily convert the models to the appropriate format (e.g. using ONXX).\n",
    "\n",
    "Here's a summary of how the contents are structured:\n",
    "\n",
    "- `purchase100_lo`\n",
    "  - `train`\n",
    "      - `model_0`\n",
    "        - `model.pt`: Serialized model weights\n",
    "        - `seed_challenge`: PRNG seed used to select a list of 100 challenge examples\n",
    "        - `seed_training`: PRNG seed used to select the non-challenge examples in the training dataset\n",
    "        - `seed_membership`: PRNG seed used to split the set of challenge examples into members and non-members (100 of each)\n",
    "        - `solution.csv`: Membership information of the challenge examples (`1` for member, `0` for non-member)\n",
    "      - ...\n",
    "  - `dev`\n",
    "      - `model_100`\n",
    "        - `model.pt`\n",
    "        - `seed_challenge`\n",
    "      - ...\n",
    "  - `final`\n",
    "    - `model_150`\n",
    "      - `model.pt`\n",
    "      - `seed_challenge`\n",
    "    - ...\n",
    "- `purchase100_hi`\n",
    "  - ...\n",
    "- `purchase100_inf`\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Your task as a competitor is to produce, for each model in `dev` and `final`, a CSV file listing your confidence scores (values between 0 and 1) for the membership of the challenge examples. You must save these scores in a `prediction.csv` file and place it in the same folder as the corresponding model. A submission to the challenge is an an archive containing just these `prediction.csv` files.\n",
    "\n",
    "**You must submit predictions for both `dev` and `final` when you submit to CodaLab.**\n",
    "\n",
    "In the following, we will show you how to compute predictions from a basic membership inference attack and package them as a submission archive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from mico_competition import ChallengeDataset, load_purchase100, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Purchase-100 dataset not found.\nYou may download the dataset from https://www.comp.nus.edu.sg/~reza/files/datasets.html\nand unzip it in the ./purchase100 directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m scenarios \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(CHALLENGE)\n\u001b[1;32m      6\u001b[0m phases \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_purchase100\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scenario \u001b[38;5;129;01min\u001b[39;00m tqdm(scenarios, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscenario\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/MICO/src/mico_competition/challenge_datasets.py:99\u001b[0m, in \u001b[0;36mload_purchase100\u001b[0;34m(dataset_dir)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_purchase100\u001b[39m(dataset_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"Loads the Purchase-100 dataset.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPurchase100\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MICO/src/mico_competition/challenge_datasets.py:70\u001b[0m, in \u001b[0;36mPurchase100.__init__\u001b[0;34m(self, dataset_dir)\u001b[0m\n\u001b[1;32m     67\u001b[0m dataset_path_pickle \u001b[38;5;241m=\u001b[39m dataset_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path_pickle):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPurchase-100 dataset not found.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may download the dataset from https://www.comp.nus.edu.sg/~reza/files/datasets.html\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand unzip it in the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/purchase100 directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path_pickle):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound the dataset. Saving it in a pickle file that takes less time to load...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Purchase-100 dataset not found.\nYou may download the dataset from https://www.comp.nus.edu.sg/~reza/files/datasets.html\nand unzip it in the ./purchase100 directory"
     ]
    }
   ],
   "source": [
    "CHALLENGE = \"provided_data/purchase100\"\n",
    "LEN_TRAINING = 150000\n",
    "LEN_CHALLENGE = 100\n",
    "\n",
    "scenarios = os.listdir(CHALLENGE)\n",
    "phases = ['dev', 'final', 'train']\n",
    "\n",
    "dataset = load_purchase100()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "for scenario in tqdm(scenarios, desc=\"scenario\"):\n",
    "    for phase in tqdm(phases, desc=\"phase\"):\n",
    "        root = os.path.join(CHALLENGE, scenario, phase)\n",
    "        for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            challenge_dataset = ChallengeDataset.from_path(path, dataset=dataset, len_training=LEN_TRAINING)\n",
    "            challenge_points = challenge_dataset.get_challenges()\n",
    "\n",
    "            # This is where you plug in your membership inference attack\n",
    "            # As an example, here is a simple loss threshold attack\n",
    "\n",
    "            # Loss Threshold Attack\n",
    "            model = load_model('purchase100', path)\n",
    "            challenge_dataloader = torch.utils.data.DataLoader(challenge_points, batch_size=2*LEN_CHALLENGE)\n",
    "            features, labels = next(iter(challenge_dataloader))\n",
    "            output = model(features)\n",
    "            predictions = -criterion(output, labels).detach().numpy()\n",
    "            # Normalize to unit interval\n",
    "            min_prediction = np.min(predictions)\n",
    "            max_prediction = np.max(predictions)\n",
    "            predictions = (predictions - min_prediction) / (max_prediction - min_prediction)\n",
    "\n",
    "            assert np.all((0 <= predictions) & (predictions <= 1))\n",
    "\n",
    "            with open(os.path.join(path, \"prediction.csv\"), \"w\") as f:\n",
    "                 csv.writer(f).writerow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Let's see how the attack does on `train`, for which we have the ground truth. \n",
    "When preparing a submission, you can use part of `train` to develop an attack and a held-out part to evaluate your attack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mico_competition.scoring import tpr_at_fpr, score, generate_roc, generate_table\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "FPR_THRESHOLD = 0.1\n",
    "\n",
    "all_scores = {}\n",
    "phases = ['train']\n",
    "\n",
    "for scenario in tqdm(scenarios, desc=\"scenario\"): \n",
    "    all_scores[scenario] = {}    \n",
    "    for phase in tqdm(phases, desc=\"phase\"):\n",
    "        predictions = []\n",
    "        solutions  = []\n",
    "\n",
    "        root = os.path.join(CHALLENGE, scenario, phase)\n",
    "        for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\"), delimiter=\",\"))\n",
    "            solutions.append(np.loadtxt(os.path.join(path, \"solution.csv\"),   delimiter=\",\"))\n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        solutions = np.concatenate(solutions)\n",
    "        \n",
    "        scores = score(solutions, predictions)\n",
    "        all_scores[scenario][phase] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the ROC curve for the attack and see how the attack performed on different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "for scenario in scenarios:\n",
    "    fpr = all_scores[scenario]['train']['fpr']\n",
    "    tpr = all_scores[scenario]['train']['tpr']\n",
    "    fig = generate_roc(fpr, tpr)\n",
    "    fig.suptitle(f\"{scenario}\", x=-0.1, y=0.5)\n",
    "    fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(scenario)\n",
    "    scores = all_scores[scenario]['train']\n",
    "    scores.pop('fpr', None)\n",
    "    scores.pop('tpr', None)\n",
    "    display(pd.DataFrame([scores]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging the submission\n",
    "\n",
    "Now we can store the predictions into a zip file, which you can submit to CodaLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "phases = ['dev', 'final']\n",
    "\n",
    "with zipfile.ZipFile(\"predictions_purchase100.zip\", 'w') as zipf:\n",
    "    for scenario in tqdm(scenarios, desc=\"scenario\"): \n",
    "        for phase in tqdm(phases, desc=\"phase\"):\n",
    "            root = os.path.join(CHALLENGE, scenario, phase)\n",
    "            for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                file = os.path.join(path, \"prediction.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    zipf.write(file)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`prediction.csv` not found in {path}. You need to provide predictions for all challenges\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c823568a0650a753a55947c22141ec594c2fc02bd68b5a71e505ecc57f17796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
